{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11ff7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc93def5",
   "metadata": {},
   "source": [
    "Tout d'abord on crée la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1090b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, U, V, mu, nu, q, epsilon):\n",
    "        self.U = U  # (nb_receivers, nb_states, nb_actions)\n",
    "        self.V = V  # (nb_states, nb_actions)\n",
    "        self.mu = mu  # (nb_states,)\n",
    "        self.nu = nu  # (nb_receivers, nb_states)\n",
    "        self.q = q  # (nb_receivers, nb_messages, nb_actions)\n",
    "        self.epsilon = epsilon  # (nb_receivers,)\n",
    "        self.nb_receivers, self.nb_states, self.nb_actions = U.shape\n",
    "        self.nb_messages = q.shape[1]\n",
    "        self.size = self.nb_receivers * self.nb_states * self.nb_messages\n",
    "        self.shape = (self.nb_receivers, self.nb_states, self.nb_messages)\n",
    "        self.check()\n",
    "    \n",
    "    def check(self):\n",
    "        for receiver_idx in range(self.nb_receivers):\n",
    "            self.debug_shape(self.U[receiver_idx], [self.nb_states, self.nb_actions])\n",
    "            self.debug_shape(self.V, [self.nb_states, self.nb_actions])\n",
    "            self.debug_shape(self.mu, [self.nb_states])\n",
    "            self.debug_shape(self.nu[receiver_idx], [self.nb_states])\n",
    "            self.debug_shape(self.q[receiver_idx], [self.nb_messages, self.nb_actions])\n",
    "\n",
    "    def debug_shape(self, vect, target_shape):\n",
    "        if list(vect.shape) != target_shape:\n",
    "            print(f\"Found vector of size {vect.shape}, expected {target_shape}\")\n",
    "            assert False\n",
    "    \n",
    "    def verbose(self, pi):\n",
    "        print(f\"We have (state, message, action) = ({self.nb_states}, {self.nb_messages}, {self.nb_actions})\")\n",
    "        self.debug_shape(pi, [self.nb_states, self.nb_messages])\n",
    "        for receiver_idx in range(self.nb_receivers):\n",
    "            theta = self.compute_theta(self.compute_g(pi, receiver_idx), receiver_idx)\n",
    "            self.debug_shape(theta, [self.nb_messages, self.nb_actions])\n",
    "            print(f\"We have 1 = {pi.sum(axis=1)}\")\n",
    "            print(f\"We have 1 = {theta.sum(axis=1)}\")\n",
    "            print(\"Theta\", theta)\n",
    "        print(\"Objective\", self.objective(pi, range(self.nb_receivers)))\n",
    "    \n",
    "    def compute_g(self, pi, receiver_idx):\n",
    "        denominator = (pi * self.nu[receiver_idx][:, None]).sum(axis=0)\n",
    "        self.debug_shape(denominator, [self.nb_messages])\n",
    "        g = (pi[:, :, None] * self.nu[receiver_idx][:, None, None] * self.U[receiver_idx][:, None, :]).sum(axis=0)\n",
    "        self.debug_shape(g, [self.nb_messages, self.nb_actions])\n",
    "        return g / denominator[:, None]\n",
    "\n",
    "    def compute_theta(self, g, receiver_idx):\n",
    "        max_g, _ = g.max(axis=1)\n",
    "        exp = torch.exp((g - max_g[:, None]) / self.epsilon[receiver_idx])\n",
    "        self.debug_shape(exp, [self.nb_messages, self.nb_actions])\n",
    "        theta = self.q[receiver_idx] * exp\n",
    "        denom = theta.sum(axis=1)\n",
    "        return theta / denom[:, None]\n",
    "\n",
    "    def objective(self, pi, receivers_batch):\n",
    "        total_objective = 0\n",
    "        for receiver_idx in receivers_batch:\n",
    "            g = self.compute_g(pi, receiver_idx)\n",
    "            theta = self.compute_theta(g, receiver_idx)\n",
    "            total_objective += (theta[None, :, :] * pi[:, :, None] * self.mu[:, None, None] * self.V[:, None, :]).sum()\n",
    "        return total_objective / len(receivers_batch)\n",
    "\n",
    "    def value(self, x, receivers_batch=None):\n",
    "        if receivers_batch is None:\n",
    "            receivers_batch = range(self.nb_receivers)\n",
    "        x = x.reshape(self.nb_states, self.nb_messages)\n",
    "        x = torch.from_numpy(x).requires_grad_(True)\n",
    "        f = self.objective(x, receivers_batch)\n",
    "        f.backward()\n",
    "        df = x.grad\n",
    "        return -f.item(), -df.numpy()\n",
    "\n",
    "    def project(self, x):\n",
    "        x = torch.from_numpy(x).reshape(self.nb_states, self.nb_messages)\n",
    "        x_projected = torch.zeros_like(x)\n",
    "        for i in range(x.shape[0]):\n",
    "            row = x[i, :]\n",
    "            sorted_row, _ = torch.sort(row, descending=True)\n",
    "            cumulative_sum = torch.cumsum(sorted_row, dim=0)\n",
    "            rho = torch.nonzero(sorted_row * torch.arange(1, len(row) + 1) > (cumulative_sum - 1), as_tuple=False).max()\n",
    "            theta = (cumulative_sum[rho] - 1) / (rho + 1)\n",
    "            x_projected[i, :] = torch.clamp(row - theta, min=0)\n",
    "        return x_projected.numpy()\n",
    "\n",
    "    def project_tangent(self, x, d):\n",
    "        d2 = d - d.mean(axis=1)[:, None]\n",
    "        d2[(x == 0) * (d2 < 0)] = 0.\n",
    "        d2[(x == 1) * (d2 > 0)] = 0.\n",
    "        return d2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2add6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dot(a, b):\n",
    "    return (a * b).sum()\n",
    "\n",
    "def ls_wolfe(x, function, step, descent, f, df, batch):\n",
    "    \"\"\"\n",
    "    Wolfe line search for stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    step_min, step_max = 0., np.inf\n",
    "    scal = dot(df, descent)\n",
    "    if scal > 0:\n",
    "        print('WARNING with scal', scal)\n",
    "    step2 = step\n",
    "    eps1, eps2 = 1.e-4, 0.9  # Wolfe condition parameters\n",
    "    i = 0\n",
    "    while i < 100:\n",
    "        i += 1\n",
    "        x2 = function.project(x + step2 * descent)\n",
    "        f2, df2 = function.value(x2, batch)\n",
    "        if dot(x2 - x, df) >= 0:\n",
    "            print('We have a problem', dot(x2 - x, df), dot(descent, df))\n",
    "        if f2 > f + eps1 * dot(x2 - x, df):  # step is too big, decrease it\n",
    "            step_max = step2\n",
    "            step2 = 0.5 * (step_min + step_max)\n",
    "        else:\n",
    "            if dot(df2, x2 - x) < eps2 * dot(df, x2 - x):  # step is too small, increase it\n",
    "                step_min = step2\n",
    "                step2 = min(0.5 * (step_min + step_max), 2 * step_min)\n",
    "            else:\n",
    "                return x2, f2, df2, step2\n",
    "    print('We do not exit Wolfe')\n",
    "    return x2, f2, df2, step2\n",
    "\n",
    "\n",
    "def dot(a,b) :\n",
    "    return (a*b).sum()\n",
    "\n",
    "def ls_wolfe(x,function,step,descent,f,df,batch) :\n",
    "    step_min,step_max=0.,np.inf\n",
    "    scal=dot(df,descent)\n",
    "    if scal > 0 :\n",
    "        print('WARNING with scal',scal)\n",
    "    step2=step\n",
    "    eps1,eps2=1.e-4,0.9\n",
    "    i=0\n",
    "    while i<100 :\n",
    "        i=i+1\n",
    "        x2=function.project(x+step2*descent)\n",
    "        f2,df2=function.value(x2,batch)\n",
    "        if dot(x2-x,df) >=0 :\n",
    "            print('We have a problem',dot(x2-x,df),dot(descent,df))\n",
    "        if f2>f+eps1*dot(x2-x,df) : # step is too big, decrease it\n",
    "            step_max=step2\n",
    "            step2=0.5*(step_min+step_max)\n",
    "        else :\n",
    "            if dot(df2,x2-x) < eps2*dot(df,x2-x) : # step is too small, increase it\n",
    "                step_min=step2\n",
    "                step2=min(0.5*(step_min+step_max),2*step_min)\n",
    "            else :\n",
    "                return x2,f2,df2,step2\n",
    "    print('We do not exit Wolfe')\n",
    "    print(f2>f+eps1*step2*scal,dot(df2,descent) < eps2*scal)\n",
    "    return x2,f2,df2,step2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize(function,itermax = 5000,tol=1.e-6,batch_size = 100,verbose=True):\n",
    "    np.random.seed(42)\n",
    "    receivers = list(range(function.nb_receivers))\n",
    "    x = np.random.randn(function.nb_states, function.nb_messages)\n",
    "    x=function.project(x)\n",
    "    np.random.seed(None)\n",
    "    list_costs=[]\n",
    "    list_grads=[]\n",
    "    nbiter = 0\n",
    "    batch = np.random.choice(receivers, size=batch_size, replace=False)\n",
    "    f,df=function.value(x,batch)\n",
    "    df_tangent=function.project_tangent(x,-df)\n",
    "    norm_grad=np.linalg.norm(df_tangent)\n",
    "    err=2*tol\n",
    "    if verbose :\n",
    "        print('iter={:4d} f={:1.3e} df={:1.3e}'.format(nbiter,f,err))\n",
    "    list_costs.append(f)\n",
    "    list_grads.append(norm_grad)\n",
    "    while (err > tol) and (nbiter < itermax):\n",
    "        descent=-df\n",
    "        x_old=np.copy(x)\n",
    "        x,f,df,step = ls_wolfe(x, function,1., descent,f,df,batch)\n",
    "        batch = np.random.choice(receivers, size=batch_size, replace=False)\n",
    "        norm_grad = np.linalg.norm(function.project_tangent(x,-df))\n",
    "        list_costs.append(f)\n",
    "        list_grads.append(norm_grad)\n",
    "        err=norm_grad\n",
    "        nbiter+=1\n",
    "        if verbose :\n",
    "            print('iter={:4d} f={:1.3e} err={:1.3e} s={:1.3e}'.format(nbiter,f,err,step))\n",
    "        if (err <= tol):\n",
    "            if verbose : print(\"Success !!! Algorithm converged !!!\")\n",
    "            return x,list_costs,list_grads\n",
    "    if verbose : print(\"FAILED to converge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd445d3b",
   "metadata": {},
   "source": [
    "On crée 1000 variations d'un receiver de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d418afb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de U : tensor([[ 1.0084, -0.0143],\n",
      "        [ 0.0013,  1.0145]])\n",
      "Exemple de nu (somme = 1) : tensor([0.6897, 0.3103]) Somme = tensor(1.)\n",
      "Exemple de q (lignes identiques, somme = 1) : tensor([[0.3015, 0.6985],\n",
      "        [0.3015, 0.6985]]) Somme des lignes = tensor([1., 1.])\n",
      "Exemple de epsilon : tensor([1.0000e-08, 6.1013e-03, 1.5562e-02, 3.6889e-03, 1.3474e-02, 2.4862e-02,\n",
      "        2.4202e-03, 2.3388e-02, 3.0227e-03, 4.6717e-03])\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "beta = 0.3\n",
    "\n",
    "# Initialisation des matrices de base\n",
    "nb_receivers = 1000\n",
    "nb_states = 2\n",
    "nb_actions = 2\n",
    "nb_messages = 2\n",
    "\n",
    "U_base = torch.tensor([[1.0, 0.0], [0.0, 1.0]], dtype=torch.float64)  # Matrice U de base\n",
    "nu_base = torch.tensor([alpha, 1 - alpha], dtype=torch.float64)  # Distribution de probabilité de base pour nu\n",
    "q_base = torch.tensor([beta, 1 - beta], dtype=torch.float64)  # Une seule ligne pour q de base\n",
    "epsilon_base = 0.01  # Valeur de base pour epsilon\n",
    "\n",
    "mu = torch.tensor([alpha, 1 - alpha])\n",
    "\n",
    "# Fonction pour ajouter du bruit à une matrice tout en conservant les propriétés des distributions\n",
    "def add_noise_to_distribution(base_vector, noise_level):\n",
    "    noisy_vector = base_vector + noise_level * torch.randn_like(base_vector)\n",
    "    noisy_vector = torch.clamp(noisy_vector, min=1e-8)  # Évite les valeurs négatives\n",
    "    return noisy_vector / noisy_vector.sum()  # Normalisation pour conserver les distributions\n",
    "\n",
    "# Génération des variations\n",
    "noise_level_U = 0.01\n",
    "noise_level_nu = 0.01\n",
    "noise_level_q = 0.01\n",
    "noise_level_epsilon = 0.01\n",
    "\n",
    "U = torch.stack([U_base + noise_level_U * torch.randn_like(U_base) for _ in range(nb_receivers)])\n",
    "nu = torch.stack([add_noise_to_distribution(nu_base, noise_level_nu) for _ in range(nb_receivers)])\n",
    "q = torch.stack([add_noise_to_distribution(q_base, noise_level_q).expand(nb_messages, -1) for _ in range(nb_receivers)])\n",
    "epsilon = torch.tensor([epsilon_base + noise_level_epsilon * np.random.randn() for _ in range(nb_receivers)])\n",
    "epsilon = torch.clamp(epsilon, min=1e-8)  # Assurez-vous que epsilon reste positif\n",
    "\n",
    "# Vérifications\n",
    "print(\"Exemple de U :\", U[0])\n",
    "print(\"Exemple de nu (somme = 1) :\", nu[0], \"Somme =\", nu[0].sum())\n",
    "print(\"Exemple de q (lignes identiques, somme = 1) :\", q[0], \"Somme des lignes =\", q[0].sum(dim=1))\n",
    "print(\"Exemple de epsilon :\", epsilon[:10])\n",
    "\n",
    "V = torch.tensor([[0.0, 1.0],\n",
    "                  [0.0, 1.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4aa9e",
   "metadata": {},
   "source": [
    "On optimise le \"vrai\" max (moyenne sur tous les Receivers) pour comparer ensuite avec l'optimisation sur des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50200bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=   0 f=-4.091e-01 df=2.000e-04\n",
      "iter=   1 f=-5.482e-01 err=4.075e-01 s=5.000e-01\n",
      "iter=   2 f=-5.512e-01 err=7.591e-01 s=1.250e-01\n",
      "iter=   3 f=-5.568e-01 err=1.687e-01 s=3.125e-02\n",
      "iter=   4 f=-5.575e-01 err=7.286e-02 s=6.250e-02\n",
      "iter=   5 f=-5.575e-01 err=4.994e-02 s=6.250e-02\n",
      "iter=   6 f=-5.575e-01 err=3.042e-02 s=6.250e-02\n",
      "iter=   7 f=-5.576e-01 err=2.033e-02 s=6.250e-02\n",
      "iter=   8 f=-5.576e-01 err=1.292e-02 s=6.250e-02\n",
      "iter=   9 f=-5.576e-01 err=8.510e-03 s=6.250e-02\n",
      "iter=  10 f=-5.576e-01 err=5.485e-03 s=6.250e-02\n",
      "iter=  11 f=-5.576e-01 err=3.588e-03 s=6.250e-02\n",
      "iter=  12 f=-5.576e-01 err=2.325e-03 s=6.250e-02\n",
      "iter=  13 f=-5.576e-01 err=1.516e-03 s=6.250e-02\n",
      "iter=  14 f=-5.576e-01 err=9.845e-04 s=6.250e-02\n",
      "iter=  15 f=-5.576e-01 err=6.410e-04 s=6.250e-02\n",
      "iter=  16 f=-5.576e-01 err=4.167e-04 s=6.250e-02\n",
      "iter=  17 f=-5.576e-01 err=2.712e-04 s=6.250e-02\n",
      "iter=  18 f=-5.576e-01 err=1.763e-04 s=6.250e-02\n",
      "iter=  19 f=-5.576e-01 err=1.147e-04 s=6.250e-02\n",
      "iter=  20 f=-5.576e-01 err=7.462e-05 s=6.250e-02\n",
      "Success !!! Algorithm converged !!!\n"
     ]
    }
   ],
   "source": [
    "# Création de l'objet Problem avec les données générées\n",
    "P = Problem(U, V, mu, nu, q, epsilon)\n",
    "\n",
    "x,costs,grad=optimize(P,tol=1.e-4,verbose=True,batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5d54229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61758566, 0.38241434],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0aa35a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.557565229324088,\n",
       " array([[-3.37761417e-13,  1.05524736e-04],\n",
       "        [-7.15813323e-12, -5.57605583e-01]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14c71ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=   0 f=-4.091e-01 df=2.000e-01\n",
      "iter=   1 f=-5.374e-01 err=1.685e+00 s=6.250e-01\n",
      "iter=   2 f=-5.517e-01 err=3.517e-01 s=3.125e-02\n",
      "iter=   3 f=-5.546e-01 err=6.108e-01 s=1.250e-01\n",
      "iter=   4 f=-5.547e-01 err=1.567e-01 s=3.125e-02\n",
      "iter=   5 f=-5.564e-01 err=2.060e-01 s=6.250e-02\n",
      "iter=   6 f=-5.580e-01 err=4.068e-02 s=6.250e-02\n",
      "Success !!! Algorithm converged !!!\n"
     ]
    }
   ],
   "source": [
    "P = Problem(U, V, mu, nu, q, epsilon)\n",
    "\n",
    "x,costs,grad=optimize(P,tol=1.e-1,verbose=True,batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8343a409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61619069, 0.38380931],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a5d530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5575115139988321,\n",
       " array([[-3.37761417e-13,  7.85272586e-02],\n",
       "        [-7.15813323e-12, -5.87651007e-01]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fc23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
