{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11ff7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc93def5",
   "metadata": {},
   "source": [
    "Tout d'abord on crée la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1090b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, U, V, mu, nu, q, epsilon):\n",
    "        self.U = U  # (nb_receivers, nb_states, nb_actions)\n",
    "        self.V = V  # (nb_states, nb_actions)\n",
    "        self.mu = mu  # (nb_states,)\n",
    "        self.nu = nu  # (nb_receivers, nb_states)\n",
    "        self.q = q  # (nb_receivers, nb_messages, nb_actions)\n",
    "        self.epsilon = epsilon  # (nb_receivers,)\n",
    "        self.nb_receivers, self.nb_states, self.nb_actions = U.shape\n",
    "        self.nb_messages = q.shape[1]\n",
    "        self.size = self.nb_receivers * self.nb_states * self.nb_messages\n",
    "        self.shape = (self.nb_receivers, self.nb_states, self.nb_messages)\n",
    "        self.check()\n",
    "    \n",
    "    def check(self):\n",
    "        for receiver_idx in range(self.nb_receivers):\n",
    "            self.debug_shape(self.U[receiver_idx], [self.nb_states, self.nb_actions])\n",
    "            self.debug_shape(self.V, [self.nb_states, self.nb_actions])\n",
    "            self.debug_shape(self.mu, [self.nb_states])\n",
    "            self.debug_shape(self.nu[receiver_idx], [self.nb_states])\n",
    "            self.debug_shape(self.q[receiver_idx], [self.nb_messages, self.nb_actions])\n",
    "\n",
    "    def debug_shape(self, vect, target_shape):\n",
    "        if list(vect.shape) != target_shape:\n",
    "            print(f\"Found vector of size {vect.shape}, expected {target_shape}\")\n",
    "            assert False\n",
    "    \n",
    "    def verbose(self, pi):\n",
    "        print(f\"We have (state, message, action) = ({self.nb_states}, {self.nb_messages}, {self.nb_actions})\")\n",
    "        self.debug_shape(pi, [self.nb_states, self.nb_messages])\n",
    "        for receiver_idx in range(self.nb_receivers):\n",
    "            theta = self.compute_theta(self.compute_g(pi, receiver_idx), receiver_idx)\n",
    "            self.debug_shape(theta, [self.nb_messages, self.nb_actions])\n",
    "            print(f\"We have 1 = {pi.sum(axis=1)}\")\n",
    "            print(f\"We have 1 = {theta.sum(axis=1)}\")\n",
    "            print(\"Theta\", theta)\n",
    "        print(\"Objective\", self.objective(pi, range(self.nb_receivers)))\n",
    "    \n",
    "    def compute_g(self, pi, receiver_idx):\n",
    "        denominator = (pi * self.nu[receiver_idx][:, None]).sum(axis=0)\n",
    "        self.debug_shape(denominator, [self.nb_messages])\n",
    "        g = (pi[:, :, None] * self.nu[receiver_idx][:, None, None] * self.U[receiver_idx][:, None, :]).sum(axis=0)\n",
    "        self.debug_shape(g, [self.nb_messages, self.nb_actions])\n",
    "        return g / denominator[:, None]\n",
    "\n",
    "    def compute_theta(self, g, receiver_idx):\n",
    "        max_g, _ = g.max(axis=1)\n",
    "        exp = torch.exp((g - max_g[:, None]) / self.epsilon[receiver_idx])\n",
    "        self.debug_shape(exp, [self.nb_messages, self.nb_actions])\n",
    "        theta = self.q[receiver_idx] * exp\n",
    "        denom = theta.sum(axis=1)\n",
    "        return theta / denom[:, None]\n",
    "\n",
    "    def objective(self, pi, receivers_batch):\n",
    "        total_objective = 0\n",
    "        for receiver_idx in receivers_batch:\n",
    "            g = self.compute_g(pi, receiver_idx)\n",
    "            theta = self.compute_theta(g, receiver_idx)\n",
    "            total_objective += (theta[None, :, :] * pi[:, :, None] * self.mu[:, None, None] * self.V[:, None, :]).sum()\n",
    "        return total_objective / len(receivers_batch)\n",
    "\n",
    "    def value(self, x, receivers_batch=None):\n",
    "        if receivers_batch is None:\n",
    "            receivers_batch = range(self.nb_receivers)\n",
    "        x = x.reshape(self.nb_states, self.nb_messages)\n",
    "        x = torch.from_numpy(x).requires_grad_(True)\n",
    "        f = self.objective(x, receivers_batch)\n",
    "        f.backward()\n",
    "        df = x.grad\n",
    "        return -f.item(), -df.numpy()\n",
    "\n",
    "    def project(self, x):\n",
    "        x = torch.from_numpy(x).reshape(self.nb_states, self.nb_messages)\n",
    "        x_projected = torch.zeros_like(x)\n",
    "        for i in range(x.shape[0]):\n",
    "            row = x[i, :]\n",
    "            sorted_row, _ = torch.sort(row, descending=True)\n",
    "            cumulative_sum = torch.cumsum(sorted_row, dim=0)\n",
    "            rho = torch.nonzero(sorted_row * torch.arange(1, len(row) + 1) > (cumulative_sum - 1), as_tuple=False).max()\n",
    "            theta = (cumulative_sum[rho] - 1) / (rho + 1)\n",
    "            x_projected[i, :] = torch.clamp(row - theta, min=0)\n",
    "        return x_projected.numpy()\n",
    "\n",
    "    def project_tangent(self, x, d):\n",
    "        d2 = d - d.mean(axis=1)[:, None]\n",
    "        d2[(x == 0) * (d2 < 0)] = 0.\n",
    "        d2[(x == 1) * (d2 > 0)] = 0.\n",
    "        return d2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2add6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dot(a, b):\n",
    "    return (a * b).sum()\n",
    "\n",
    "def ls_wolfe(x, function, step, descent, f, df, batch):\n",
    "    \"\"\"\n",
    "    Wolfe line search for stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    step_min, step_max = 0., np.inf\n",
    "    scal = dot(df, descent)\n",
    "    if scal > 0:\n",
    "        print('WARNING with scal', scal)\n",
    "    step2 = step\n",
    "    eps1, eps2 = 1.e-4, 0.9  # Wolfe condition parameters\n",
    "    i = 0\n",
    "    while i < 100:\n",
    "        i += 1\n",
    "        x2 = function.project(x + step2 * descent)\n",
    "        f2, df2 = function.value(x2, batch)\n",
    "        if dot(x2 - x, df) >= 0:\n",
    "            print('We have a problem', dot(x2 - x, df), dot(descent, df))\n",
    "        if f2 > f + eps1 * dot(x2 - x, df):  # step is too big, decrease it\n",
    "            step_max = step2\n",
    "            step2 = 0.5 * (step_min + step_max)\n",
    "        else:\n",
    "            if dot(df2, x2 - x) < eps2 * dot(df, x2 - x):  # step is too small, increase it\n",
    "                step_min = step2\n",
    "                step2 = min(0.5 * (step_min + step_max), 2 * step_min)\n",
    "            else:\n",
    "                return x2, f2, df2, step2\n",
    "    print('We do not exit Wolfe')\n",
    "    return x2, f2, df2, step2\n",
    "\n",
    "\n",
    "def dot(a,b) :\n",
    "    return (a*b).sum()\n",
    "\n",
    "def ls_wolfe(x,function,step,descent,f,df,batch) :\n",
    "    step_min,step_max=0.,np.inf\n",
    "    scal=dot(df,descent)\n",
    "    if scal > 0 :\n",
    "        print('WARNING with scal',scal)\n",
    "    step2=step\n",
    "    eps1,eps2=1.e-4,0.9\n",
    "    i=0\n",
    "    while i<100 :\n",
    "        i=i+1\n",
    "        x2=function.project(x+step2*descent)\n",
    "        f2,df2=function.value(x2,batch)\n",
    "        if dot(x2-x,df) >=0 :\n",
    "            print('We have a problem',dot(x2-x,df),dot(descent,df))\n",
    "        if f2>f+eps1*dot(x2-x,df) : # step is too big, decrease it\n",
    "            step_max=step2\n",
    "            step2=0.5*(step_min+step_max)\n",
    "        else :\n",
    "            if dot(df2,x2-x) < eps2*dot(df,x2-x) : # step is too small, increase it\n",
    "                step_min=step2\n",
    "                step2=min(0.5*(step_min+step_max),2*step_min)\n",
    "            else :\n",
    "                return x2,f2,df2,step2\n",
    "    print('We do not exit Wolfe')\n",
    "    print(f2>f+eps1*step2*scal,dot(df2,descent) < eps2*scal)\n",
    "    return x2,f2,df2,step2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize(function,itermax = 5000,tol=1.e-6,batch_size = 100,verbose=True):\n",
    "    np.random.seed(42)\n",
    "    receivers = list(range(function.nb_receivers))\n",
    "    x = np.random.randn(function.nb_states, function.nb_messages)\n",
    "    x=function.project(x)\n",
    "    np.random.seed(None)\n",
    "    list_costs=[]\n",
    "    list_grads=[]\n",
    "    nbiter = 0\n",
    "    batch = np.random.choice(receivers, size=batch_size, replace=False)\n",
    "    f,df=function.value(x,batch)\n",
    "    df_tangent=function.project_tangent(x,-df)\n",
    "    norm_grad=np.linalg.norm(df_tangent)\n",
    "    err=2*tol\n",
    "    if verbose :\n",
    "        print('iter={:4d} f={:1.3e} df={:1.3e}'.format(nbiter,f,err))\n",
    "    list_costs.append(f)\n",
    "    list_grads.append(norm_grad)\n",
    "    while (err > tol) and (nbiter < itermax):\n",
    "        descent=-df\n",
    "        x_old=np.copy(x)\n",
    "        x,f,df,step = ls_wolfe(x, function,1., descent,f,df,batch)\n",
    "        batch = np.random.choice(receivers, size=batch_size, replace=False)\n",
    "        norm_grad = np.linalg.norm(function.project_tangent(x,-df))\n",
    "        list_costs.append(f)\n",
    "        list_grads.append(norm_grad)\n",
    "        err=norm_grad\n",
    "        nbiter+=1\n",
    "        if verbose :\n",
    "            print('iter={:4d} f={:1.3e} err={:1.3e} s={:1.3e}'.format(nbiter,f,err,step))\n",
    "        if (err <= tol):\n",
    "            if verbose : print(\"Success !!! Algorithm converged !!!\")\n",
    "            return x,list_costs,list_grads\n",
    "    if verbose : print(\"FAILED to converge\")\n",
    "        \n",
    "\n",
    "def stochastic_optimize(function,itermax = 5000,tol=1.e-6,batch_size = 100,verbose=True):\n",
    "    np.random.seed(42)\n",
    "    receivers = list(range(function.nb_receivers))\n",
    "    x = np.random.randn(function.nb_states, function.nb_messages)\n",
    "    x=function.project(x)\n",
    "    np.random.seed(None)\n",
    "    list_costs=[]\n",
    "    list_grads=[]\n",
    "    nbiter = 0\n",
    "    batch = np.random.choice(receivers, size=batch_size, replace=False)\n",
    "    f,df=function.value(x,batch)\n",
    "    df_tangent=function.project_tangent(x,-df)\n",
    "    norm_grad=np.linalg.norm(df_tangent)\n",
    "    err=2*tol\n",
    "    if verbose :\n",
    "        print('iter={:4d} f={:1.3e} df={:1.3e}'.format(nbiter,f,err))\n",
    "    list_costs.append(f)\n",
    "    list_grads.append(norm_grad)\n",
    "    while (err > tol) and (nbiter < itermax):\n",
    "        nbiter+=1\n",
    "        x_old=np.copy(x)\n",
    "        batch = np.random.choice(receivers, size=batch_size, replace=False)\n",
    "        x = function.project(x - (1/nbiter)*df)\n",
    "        f,df = function.value(x,batch)\n",
    "        norm_grad = np.linalg.norm(function.project_tangent(x,-df))\n",
    "        list_costs.append(f)\n",
    "        list_grads.append(norm_grad)\n",
    "        err=norm_grad\n",
    "        if verbose :\n",
    "            print('iter={:4d} f={:1.3e} err={:1.3e}'.format(nbiter,f,err))\n",
    "        if (err <= tol):\n",
    "            if verbose : print(\"Success !!! Algorithm converged !!!\")\n",
    "            return x,list_costs,list_grads\n",
    "    if verbose : print(\"FAILED to converge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd445d3b",
   "metadata": {},
   "source": [
    "On crée 1000 variations d'un receiver de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d418afb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de U : tensor([[1.0049, 0.0118],\n",
      "        [0.0071, 0.9800]])\n",
      "Exemple de nu (somme = 1) : tensor([0.6883, 0.3117]) Somme = tensor(1.)\n",
      "Exemple de q (lignes identiques, somme = 1) : tensor([[0.3039, 0.6961],\n",
      "        [0.3039, 0.6961]]) Somme des lignes = tensor([1., 1.])\n",
      "Exemple de epsilon : tensor([1.8902e-02, 8.7466e-03, 1.0000e-08, 9.4291e-03, 1.4657e-02, 4.0578e-04,\n",
      "        1.8641e-02, 1.9156e-02, 2.7247e-02, 6.3527e-03])\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "beta = 0.3\n",
    "\n",
    "# Initialisation des matrices de base\n",
    "nb_receivers = 1000\n",
    "nb_states = 2\n",
    "nb_actions = 2\n",
    "nb_messages = 2\n",
    "\n",
    "U_base = torch.tensor([[1.0, 0.0], [0.0, 1.0]], dtype=torch.float64)  # Matrice U de base\n",
    "nu_base = torch.tensor([alpha, 1 - alpha], dtype=torch.float64)  # Distribution de probabilité de base pour nu\n",
    "q_base = torch.tensor([beta, 1 - beta], dtype=torch.float64)  # Une seule ligne pour q de base\n",
    "epsilon_base = 0.01  # Valeur de base pour epsilon\n",
    "\n",
    "mu = torch.tensor([alpha, 1 - alpha])\n",
    "\n",
    "# Fonction pour ajouter du bruit à une matrice tout en conservant les propriétés des distributions\n",
    "def add_noise_to_distribution(base_vector, noise_level):\n",
    "    noisy_vector = base_vector + noise_level * torch.randn_like(base_vector)\n",
    "    noisy_vector = torch.clamp(noisy_vector, min=1e-8)  # Évite les valeurs négatives\n",
    "    return noisy_vector / noisy_vector.sum()  # Normalisation pour conserver les distributions\n",
    "\n",
    "# Génération des variations\n",
    "noise_level_U = 0.01\n",
    "noise_level_nu = 0.01\n",
    "noise_level_q = 0.01\n",
    "noise_level_epsilon = 0.01\n",
    "\n",
    "U = torch.stack([U_base + noise_level_U * torch.randn_like(U_base) for _ in range(nb_receivers)])\n",
    "nu = torch.stack([add_noise_to_distribution(nu_base, noise_level_nu) for _ in range(nb_receivers)])\n",
    "q = torch.stack([add_noise_to_distribution(q_base, noise_level_q).expand(nb_messages, -1) for _ in range(nb_receivers)])\n",
    "epsilon = torch.tensor([epsilon_base + noise_level_epsilon * np.random.randn() for _ in range(nb_receivers)])\n",
    "epsilon = torch.clamp(epsilon, min=1e-8)  # Assurez-vous que epsilon reste positif\n",
    "\n",
    "# Vérifications\n",
    "print(\"Exemple de U :\", U[0])\n",
    "print(\"Exemple de nu (somme = 1) :\", nu[0], \"Somme =\", nu[0].sum())\n",
    "print(\"Exemple de q (lignes identiques, somme = 1) :\", q[0], \"Somme des lignes =\", q[0].sum(dim=1))\n",
    "print(\"Exemple de epsilon :\", epsilon[:10])\n",
    "\n",
    "V = torch.tensor([[0.0, 1.0],\n",
    "                  [0.0, 1.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4aa9e",
   "metadata": {},
   "source": [
    "On optimise la \"vraie\" fonction (moyenne sur tous les Receivers) pour comparer ensuite avec l'optimisation sur des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "50200bc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=   0 f=-4.091e-01 df=2.000e-05\n",
      "iter=   1 f=-6.257e-03 err=1.583e-01\n",
      "iter=   2 f=-5.535e-02 err=1.582e+00\n",
      "iter=   3 f=-3.726e-01 err=4.950e-01\n",
      "iter=   4 f=-4.338e-01 err=4.950e-01\n",
      "iter=   5 f=-4.828e-01 err=4.947e-01\n",
      "iter=   6 f=-5.235e-01 err=4.877e-01\n",
      "iter=   7 f=-5.545e-01 err=3.254e-01\n",
      "iter=   8 f=-5.451e-01 err=1.217e+00\n",
      "iter=   9 f=-5.112e-01 err=4.924e-01\n",
      "iter=  10 f=-5.352e-01 err=4.745e-01\n",
      "iter=  11 f=-5.538e-01 err=3.437e-01\n",
      "iter=  12 f=-5.573e-01 err=2.152e-01\n",
      "iter=  13 f=-5.572e-01 err=2.049e-01\n",
      "iter=  14 f=-5.577e-01 err=1.578e-01\n",
      "iter=  15 f=-5.579e-01 err=1.226e-01\n",
      "iter=  16 f=-5.580e-01 err=8.612e-02\n",
      "iter=  17 f=-5.581e-01 err=5.457e-02\n",
      "iter=  18 f=-5.582e-01 err=3.277e-02\n",
      "iter=  19 f=-5.582e-01 err=1.679e-02\n",
      "iter=  20 f=-5.582e-01 err=7.637e-03\n",
      "iter=  21 f=-5.582e-01 err=2.908e-03\n",
      "iter=  22 f=-5.582e-01 err=9.342e-04\n",
      "iter=  23 f=-5.582e-01 err=2.455e-04\n",
      "iter=  24 f=-5.582e-01 err=5.167e-05\n",
      "iter=  25 f=-5.582e-01 err=8.371e-06\n",
      "Success !!! Algorithm converged !!!\n"
     ]
    }
   ],
   "source": [
    "# Création de l'objet Problem avec les données générées\n",
    "P = Problem(U, V, mu, nu, q, epsilon)\n",
    "\n",
    "x,costs,grad=stochastic_optimize(P,tol=1.e-5,verbose=True,batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c5d54229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61818672, 0.38181328],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b0aa35a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5581882865231921,\n",
       " array([[-1.52216288e-12, -1.18386294e-05],\n",
       "        [-2.81973319e-11, -5.58183766e-01]]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "14c71ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=   0 f=-4.091e-01 df=2.000e-03\n",
      "iter=   1 f=-6.013e-03 err=1.726e-01\n",
      "iter=   2 f=-7.460e-02 err=2.111e+00\n",
      "iter=   3 f=-3.000e-01 err=4.950e-01\n",
      "iter=   4 f=-3.612e-01 err=4.950e-01\n",
      "iter=   5 f=-4.102e-01 err=4.950e-01\n",
      "iter=   6 f=-4.511e-01 err=4.950e-01\n",
      "iter=   7 f=-4.861e-01 err=4.947e-01\n",
      "iter=   8 f=-5.166e-01 err=4.917e-01\n",
      "iter=   9 f=-5.430e-01 err=4.625e-01\n",
      "iter=  10 f=-5.585e-01 err=1.310e-01\n",
      "iter=  11 f=-5.538e-01 err=6.029e-01\n",
      "iter=  12 f=-5.463e-01 err=4.377e-01\n",
      "iter=  13 f=-5.585e-01 err=2.590e-01\n",
      "iter=  14 f=-5.542e-01 err=7.091e-01\n",
      "iter=  15 f=-5.481e-01 err=4.168e-01\n",
      "iter=  16 f=-5.576e-01 err=2.300e-01\n",
      "iter=  17 f=-5.608e-01 err=4.543e-03\n",
      "iter=  18 f=-5.584e-01 err=8.279e-02\n",
      "iter=  19 f=-5.549e-01 err=2.291e-01\n",
      "iter=  20 f=-5.578e-01 err=3.083e-01\n",
      "iter=  21 f=-5.597e-01 err=2.483e-02\n",
      "iter=  22 f=-5.563e-01 err=1.621e-01\n",
      "iter=  23 f=-5.602e-01 err=2.008e-01\n",
      "iter=  24 f=-5.569e-01 err=1.273e-01\n",
      "iter=  25 f=-5.576e-01 err=4.779e-02\n",
      "iter=  26 f=-5.566e-01 err=5.370e-02\n",
      "iter=  27 f=-5.558e-01 err=8.069e-02\n",
      "iter=  28 f=-5.548e-01 err=3.929e-02\n",
      "iter=  29 f=-5.579e-01 err=1.007e-01\n",
      "iter=  30 f=-5.586e-01 err=6.008e-02\n",
      "iter=  31 f=-5.602e-01 err=5.275e-02\n",
      "iter=  32 f=-5.544e-01 err=2.068e-01\n",
      "iter=  33 f=-5.564e-01 err=2.471e-02\n",
      "iter=  34 f=-5.534e-01 err=5.104e-01\n",
      "iter=  35 f=-5.565e-01 err=3.342e-01\n",
      "iter=  36 f=-5.553e-01 err=9.434e-02\n",
      "iter=  37 f=-5.546e-01 err=1.180e-01\n",
      "iter=  38 f=-5.573e-01 err=1.818e-01\n",
      "iter=  39 f=-5.565e-01 err=1.824e-01\n",
      "iter=  40 f=-5.583e-01 err=2.304e-01\n",
      "iter=  41 f=-5.582e-01 err=8.958e-02\n",
      "iter=  42 f=-5.568e-01 err=5.292e-02\n",
      "iter=  43 f=-5.578e-01 err=4.840e-02\n",
      "iter=  44 f=-5.614e-01 err=1.975e-01\n",
      "iter=  45 f=-5.576e-01 err=1.163e-01\n",
      "iter=  46 f=-5.580e-01 err=5.666e-02\n",
      "iter=  47 f=-5.562e-01 err=3.903e-02\n",
      "iter=  48 f=-5.579e-01 err=1.689e-02\n",
      "iter=  49 f=-5.591e-01 err=6.723e-02\n",
      "iter=  50 f=-5.587e-01 err=2.330e-02\n",
      "iter=  51 f=-5.597e-01 err=5.896e-02\n",
      "iter=  52 f=-5.610e-01 err=4.287e-02\n",
      "iter=  53 f=-5.599e-01 err=3.217e-02\n",
      "iter=  54 f=-5.578e-01 err=1.469e-01\n",
      "iter=  55 f=-5.599e-01 err=8.517e-02\n",
      "iter=  56 f=-5.560e-01 err=2.324e-01\n",
      "iter=  57 f=-5.583e-01 err=1.749e-02\n",
      "iter=  58 f=-5.580e-01 err=4.041e-02\n",
      "iter=  59 f=-5.582e-01 err=2.839e-04\n",
      "Success !!! Algorithm converged !!!\n"
     ]
    }
   ],
   "source": [
    "y,costs,grad=stochastic_optimize(P,tol=1.e-3,verbose=True,batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8343a409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.61795114, 0.38204886],\n",
       "        [0.        , 1.        ]]),\n",
       " array([[0.61818672, 0.38181328],\n",
       "        [0.        , 1.        ]]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6a5d530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-0.5581866810809846,\n",
       "  array([[-1.52216288e-12,  1.36225261e-02],\n",
       "         [-2.81973319e-11, -5.63391152e-01]])),\n",
       " (-0.5581882865231921,\n",
       "  array([[-1.52216288e-12, -1.18386294e-05],\n",
       "         [-2.81973319e-11, -5.58183766e-01]])))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.value(y),P.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fc23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
